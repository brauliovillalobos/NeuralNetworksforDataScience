{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5l1IgK3XRfuh"
      },
      "source": [
        "# Importing Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "ENK_3fhB6UJX"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import layers, initializers, regularizers\n",
        "from tensorflow.keras import losses, metrics, optimizers, callbacks\n",
        "from tensorflow import keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSjwGtO4UVE1"
      },
      "source": [
        "# Import Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mmx55MiD6dwe",
        "outputId": "98aa755a-3180-4cc1-9758-18ff7a365535"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`TensorInfo.dtype` is deprecated. Please change your code to use NumPy with the field `TensorInfo.np_dtype` or use TensorFlow with the field `TensorInfo.tf_dtype`.\n"
          ]
        }
      ],
      "source": [
        "(train_dataset, val_dataset, test_dataset), dataset_info = tfds.load('stanford_dogs', split=['train[:80%]', 'train[80%:90%]', 'train[90%:]'], with_info=True, as_supervised=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "3zrosU3RhOwV"
      },
      "outputs": [],
      "source": [
        "def preprocess(image, label):\n",
        "    # A function that preprocesses each image by normalizing\n",
        "    # its values in [0, 1].\n",
        "    image = tf.cast(image, tf.float32) / 255.0\n",
        "    return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "TSpRG-IBqjNI"
      },
      "outputs": [],
      "source": [
        "def resize(image,label):\n",
        "  image = tf.image.resize(image, (500, 375))\n",
        "  image = tf.cast(image, tf.float32) / 255.0\n",
        "\n",
        "  return image,label "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "okFg1JCt3L2W"
      },
      "outputs": [],
      "source": [
        "def augment(image, label):\n",
        "  # We perform data augmentation in the data pipeline.\n",
        "  # As an alternative, we can add layers of image augmentation\n",
        "  # at the beginning of our model.\n",
        "  #image = tf.image.random_brightness(image, 0.1)\n",
        "  image = tf.image.random_flip_left_right(image)\n",
        "  return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "2fvKMVHaOn4u"
      },
      "outputs": [],
      "source": [
        "train_dataset = train_dataset.map(resize)\n",
        "val_dataset = val_dataset.map(resize)\n",
        "test_dataset = test_dataset.map(resize)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0QrQxROE5pz"
      },
      "source": [
        "### Filter only breeds of label 1 to 10"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "def breeds_of_interest_bas(image,label):\n",
        "  condition1 = tf.math.equal(label, 1)\n",
        "  condition2 = tf.math.equal(label, 2)\n",
        "  condition3 = tf.math.equal(label, 3)\n",
        "  condition4 = tf.math.equal(label, 4)\n",
        "  condition5 = tf.math.equal(label, 5)\n",
        "  condition6 = tf.math.equal(label, 6)\n",
        "  condition7 = tf.math.equal(label, 7)\n",
        "  condition8 = tf.math.equal(label, 8)\n",
        "  condition9 = tf.math.equal(label, 9)\n",
        "  return tf.reduce_any([condition1, condition2, condition3, condition4, condition5,\n",
        "                        condition6, condition7, condition8, condition9])\n",
        "'''"
      ],
      "metadata": {
        "id": "5rBxmfluswc-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "b3309443-9a23-482a-efc4-259977528b81"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ndef breeds_of_interest_bas(image,label):\\n  condition1 = tf.math.equal(label, 1)\\n  condition2 = tf.math.equal(label, 2)\\n  condition3 = tf.math.equal(label, 3)\\n  condition4 = tf.math.equal(label, 4)\\n  condition5 = tf.math.equal(label, 5)\\n  condition6 = tf.math.equal(label, 6)\\n  condition7 = tf.math.equal(label, 7)\\n  condition8 = tf.math.equal(label, 8)\\n  condition9 = tf.math.equal(label, 9)\\n  return tf.reduce_any([condition1, condition2, condition3, condition4, condition5,\\n                        condition6, condition7, condition8, condition9])\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "ynnPWLc2DR0Q"
      },
      "outputs": [],
      "source": [
        "def breeds_of_interest_eleg(image,label):\n",
        "  return tf.reduce_any(tf.equal(label, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "c2CWpuxka2BK"
      },
      "outputs": [],
      "source": [
        "train_dataset_filtered = train_dataset.filter(breeds_of_interest_eleg)\n",
        "val_dataset_filtered = val_dataset.filter(breeds_of_interest_eleg) \n",
        "test_dataset_filtered = test_dataset.filter(breeds_of_interest_eleg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLXk-usHi6Jf"
      },
      "source": [
        "## Basic Neural Network **without** Early Exits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "hg-v4B1ySF8t"
      },
      "outputs": [],
      "source": [
        "train_data_p = train_dataset_filtered.shuffle(1000).batch(12).map(augment)\n",
        "val_data_p = val_dataset_filtered.batch(12)\n",
        "test_data_p = test_dataset_filtered.batch(12)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "CemPhr3c3tcc"
      },
      "outputs": [],
      "source": [
        "def add_conv_block(x, n_filters):\n",
        "    # This function applies a simple \"CNN block\" to the input,\n",
        "    # built as Conv2D -> BN -> ReLU -> MaxPool2D.\n",
        "    x = layers.Conv2D(n_filters, 5, padding='same', kernel_regularizer=regularizers.L2(10e-3))(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = tf.nn.relu(x)\n",
        "    return layers.MaxPool2D(2)(x)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def classification_layer(x_inp):\n",
        "  x = layers.GlobalAvgPool2D()(x_inp) # Output shape: (None, 32)\n",
        "  x = layers.Dense(100, activation='relu')(x)\n",
        "  x = layers.Dropout(0.3)(x)\n",
        "  x = layers.Dense(10)(x)  \n",
        "  return x"
      ],
      "metadata": {
        "id": "5SUZGF68rtbI"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model():\n",
        "  # Input part\n",
        "  inp = layers.Input(shape=(500, 375, 3))\n",
        "  # First Convolutional Block     # Output: (None, 500, 375, 24)\n",
        "  x = add_conv_block(inp, 24)     \n",
        "\n",
        "  # Second Convolutional Block    # Output: (None, 250, 187, 48) \n",
        "  x = add_conv_block(x, 48)       \n",
        "\n",
        "  # Third Convolutional Block     # Output: (None, 125, 93, 96)\n",
        "  x = add_conv_block(x, 96)\n",
        "\n",
        "  # Fourth Convolutional Block    # Output: (None, 62, 46, 192)\n",
        "  x = add_conv_block(x, 192)     \n",
        "\n",
        "  x = classification_layer(x)\n",
        "  return tf.keras.Model(inputs=inp, outputs=x)"
      ],
      "metadata": {
        "id": "20ZHEjbDqSMb"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model = build_model()"
      ],
      "metadata": {
        "id": "qu6WFryHq9Qc"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJXZhDCjxyoo",
        "outputId": "a97aa423-fc6b-469a-a70f-a4c6b62e060a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 500, 375, 3)]     0         \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 500, 375, 24)      1824      \n",
            "                                                                 \n",
            " batch_normalization_4 (Batc  (None, 500, 375, 24)     96        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " tf.nn.relu_4 (TFOpLambda)   (None, 500, 375, 24)      0         \n",
            "                                                                 \n",
            " max_pooling2d_4 (MaxPooling  (None, 250, 187, 24)     0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 250, 187, 48)      28848     \n",
            "                                                                 \n",
            " batch_normalization_5 (Batc  (None, 250, 187, 48)     192       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " tf.nn.relu_5 (TFOpLambda)   (None, 250, 187, 48)      0         \n",
            "                                                                 \n",
            " max_pooling2d_5 (MaxPooling  (None, 125, 93, 48)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_6 (Conv2D)           (None, 125, 93, 96)       115296    \n",
            "                                                                 \n",
            " batch_normalization_6 (Batc  (None, 125, 93, 96)      384       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " tf.nn.relu_6 (TFOpLambda)   (None, 125, 93, 96)       0         \n",
            "                                                                 \n",
            " max_pooling2d_6 (MaxPooling  (None, 62, 46, 96)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_7 (Conv2D)           (None, 62, 46, 192)       460992    \n",
            "                                                                 \n",
            " batch_normalization_7 (Batc  (None, 62, 46, 192)      768       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " tf.nn.relu_7 (TFOpLambda)   (None, 62, 46, 192)       0         \n",
            "                                                                 \n",
            " max_pooling2d_7 (MaxPooling  (None, 31, 23, 192)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " global_average_pooling2d_2   (None, 192)              0         \n",
            " (GlobalAveragePooling2D)                                        \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 100)               19300     \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 100)               0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 10)                1010      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 628,710\n",
            "Trainable params: 627,990\n",
            "Non-trainable params: 720\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "#model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "GCVw0MTQmmVK"
      },
      "outputs": [],
      "source": [
        "cross_entropy = losses.SparseCategoricalCrossentropy(from_logits=True) #Remove this \"from_logits\" if put the softmax activation in last dense layer\n",
        "accuracy = metrics.SparseCategoricalAccuracy()\n",
        "optimizer = optimizers.Adam()\n",
        "\n",
        "# Callbacks are objects that provide additional functionalities during training,\n",
        "# allowing to plug-in things at will (in this case, we add a callback to immediately\n",
        "# terminate when a NaN value is encountered, a callback to perform early stopping,\n",
        "# and a callback to log the results for TensorBoard visualization).\n",
        "cbs = [\n",
        "    callbacks.TerminateOnNaN(),\n",
        "    callbacks.EarlyStopping(monitor='val_sparse_categorical_accuracy', patience=5, \n",
        "                            restore_best_weights=True, verbose=1),\n",
        "    callbacks.TensorBoard(log_dir='logs', update_freq=50)      \n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9usaG3Q35llv"
      },
      "outputs": [],
      "source": [
        "#model.compile(loss=cross_entropy, optimizer=optimizer, metrics=[accuracy])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wquxk7M2yZ9c",
        "outputId": "a367c9c6-58b5-4f69-d816-44138d9cf436"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "67/67 [==============================] - 175s 2s/step - loss: 2.4265 - sparse_categorical_accuracy: 0.1095 - val_loss: 2.3918 - val_sparse_categorical_accuracy: 0.1031\n",
            "Epoch 2/5\n",
            "67/67 [==============================] - 169s 2s/step - loss: 2.3547 - sparse_categorical_accuracy: 0.1294 - val_loss: 2.3539 - val_sparse_categorical_accuracy: 0.1340\n",
            "Epoch 3/5\n",
            "67/67 [==============================] - 172s 2s/step - loss: 2.3188 - sparse_categorical_accuracy: 0.1468 - val_loss: 2.3358 - val_sparse_categorical_accuracy: 0.1340\n",
            "Epoch 4/5\n",
            "67/67 [==============================] - 171s 2s/step - loss: 2.2891 - sparse_categorical_accuracy: 0.1542 - val_loss: 2.3618 - val_sparse_categorical_accuracy: 0.0825\n",
            "Epoch 5/5\n",
            "67/67 [==============================] - 168s 2s/step - loss: 2.2590 - sparse_categorical_accuracy: 0.1828 - val_loss: 2.3213 - val_sparse_categorical_accuracy: 0.1340\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f94575755e0>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "#model.fit(train_data_p, validation_data=val_data_p, epochs=5, callbacks=cbs)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Network WITH Early Exits"
      ],
      "metadata": {
        "id": "uDDCfRuzsc-Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def early_exit(x, training = True):\n",
        "  \n",
        "  x = layers.GlobalAvgPool2D()(x) #Possibly have to change names\n",
        "  x = layers.Dense(100, activation='relu')(x)\n",
        "  x = layers.Dropout(0.3)(x)\n",
        "  ee_output = layers.Dense(10, activation='softmax')(x)\n",
        "  return ee_output"
      ],
      "metadata": {
        "id": "-hoKs4srsh0_"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def early_exit_loss(y_true,y_preds):\n",
        "  scce = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "  loss_ee1 = scce(y_true, y_preds[0])\n",
        "  loss_ee2 = scce(y_true, y_preds[1])\n",
        "  loss_final = scce(y_true, y_preds[2])\n",
        "\n",
        "  return loss_final + loss_ee1*1 + loss_ee2*1"
      ],
      "metadata": {
        "id": "MuUv5c2ls_tW"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(training = True):\n",
        "  #*****INPUT******\n",
        "  inp = layers.Input(shape=(500, 375, 3))\n",
        "  #*******************************FIRST & SECOND CONVOLUTIONAL BLOCK***********************************#\n",
        "\n",
        "  x_cb1_t = add_conv_block(inp, 24)\n",
        "\n",
        "\n",
        "  x_cb2_t = add_conv_block(x_cb1_t, 48)\n",
        "\n",
        "  #******************************FIRST & SECOND CONVOLUTIONAL BLOCK************************************#\n",
        "\n",
        "\n",
        "  #-------------------------------------START FIRST EARLY EXIT--------------------------------------#\n",
        "  # Classification done always. Input --> x_cb2_t || Output --> ee1_output\n",
        "  x = layers.GlobalAvgPool2D()(x_cb2_t) \n",
        "  x = layers.Dense(100, activation='relu')(x)\n",
        "  x = layers.Dropout(0.3)(x)\n",
        "  ee1_output = layers.Dense(10, activation='softmax')(x)\n",
        "\n",
        "  # Stack to calculate Loss. All confidences of EE1 saved in piu[0]\n",
        "  piu = tf.stack(ee1_output, axis=0) \n",
        "    \n",
        "  # Only during inference... filtering!\n",
        "  if training == False: \n",
        "\n",
        "    threshold_ee1 = 0.9 \n",
        "    batch_size = 12\n",
        "    #Auxiliary tensor keeps track of the id of the images that haven't exited yet.\n",
        "    auxiliary_tensor = tf.range(batch_size) # used to sort images that take EE.\n",
        "    auxiliary_tensor = tf.reshape(auxiliary_tensor, [-1])\n",
        "    \n",
        "    #Take for each image of the Batch the category with highest confidence after the softmax.\n",
        "    max_confidence = tf.reduce_max(ee1_output, axis = -1) # TAKING RESULT OF THE EARLY EXIT 1\n",
        "\n",
        "    #Thresholding operation. \n",
        "    #New tensor: 0's where confidence < threshold --> shall be passed to subsequent layers\n",
        "    #1's where confidence > threshold --> shall NOT be passed to subsequent layers.\n",
        "    exiting_instances = tf.cast(tf.where(max_confidence < threshold_ee1, 0, 1), tf.int32)\n",
        "\n",
        "    #Select images with confidences ABOVE threshold that DON'T need to be given to subsequent layers\n",
        "    mask_exiters = tf.equal(exiting_instances, 1) #Mask those elements of batch that took the early exit\n",
        "    output = tf.boolean_mask(ee1_output, mask_exiters) #Take probability vector for the exiter instances\n",
        "\n",
        "    #Identify images that exited, to then sort them\n",
        "    sorting_tensor = tf.math.multiply(auxiliary_tensor,exiting_instances) #Make zero indexes of Batch that didn't exit\n",
        "    mask_non_zeros = tf.not_equal(sorting_tensor, 0) #Take those that aren't zero, so they exited.\n",
        "    sorting_tensor = tf.boolean_mask(sorting_tensor, mask_non_zeros) #Make 1 tensor with id of exited images.\n",
        "\n",
        "    #Update input_non_exiters with elements BELOW threshold that NEED to be given to subsequent layers\n",
        "    mask_non_exiters = tf.equal(exiting_instances, 0) \n",
        "    input_non_exiters = tf.boolean_mask(x_cb2_t, mask_non_exiters) #Take the PIXELS of images that didn't exit here.\n",
        "\n",
        "    \n",
        "    #Appending and sortering section\n",
        "    #Array containing vector of confidences for images that took the EE.\n",
        "    output_list = []\n",
        "    output_list.append(output)\n",
        "\n",
        "    #Array containing id and order of the images that took the EE.\n",
        "    sorting_list = []\n",
        "    sorting_list.append(sorting_tensor)\n",
        "\n",
        "    #Update auxiliary tensor by removing the ids of the images that took the early exit.\n",
        "    auxiliary_tensor = tf.compat.v1.setdiff1d(auxiliary_tensor, sorting_tensor,index_dtype=tf.dtypes.int32)\n",
        "    auxiliary_tensor = auxiliary_tensor[0]\n",
        "  #-------------------------------------END FIRST EARLY EXIT--------------------------------------#\n",
        "\n",
        "  #**********************************THIRD CONVOLUTIONAL BLOCK***********************************#\n",
        "  '''\n",
        "  Difference must be made. If training, the 3rd CB should take pixels of ALL the images in Batch. \n",
        "  If inference, 3rd CB takes pixels of ONLY images that did't take the 1st EE.\n",
        "  '''\n",
        "  if training == True:\n",
        "    input_3cb = x_cb2_t\n",
        "  else: \n",
        "    input_3cb = input_non_exiters\n",
        "\n",
        "  \n",
        "  x_cb3_t = add_conv_block(input_3cb, 96)\n",
        "  #**********************************THIRD CONVOLUTIONAL BLOCK***********************************#\n",
        "\n",
        "  #-------------------------------------START SECOND EARLY EXIT--------------------------------------#\n",
        "  # Classification done always. Input --> x_cb3_t || Output --> ee2_output\n",
        "  \n",
        "  x = layers.GlobalAvgPool2D()(x_cb3_t) \n",
        "  x = layers.Dense(100, activation='relu')(x)\n",
        "  x = layers.Dropout(0.3)(x)\n",
        "  ee2_output = layers.Dense(10, activation='softmax')(x)\n",
        "\n",
        "  # Stack to calculate Loss. All confidences of EE2 saved in piu[1]\n",
        "  piu = tf.stack([piu,ee2_output], axis=0) \n",
        "\n",
        "  # Only during inference... filtering!\n",
        "  if training == False: \n",
        "\n",
        "    threshold_ee2 = 0.8 \n",
        "    \n",
        "    #Take for each image of the Batch the category with highest confidence after the softmax.\n",
        "    max_confidence = tf.reduce_max(ee2_output, axis = -1) # TAKING RESULT OF THE EARLY EXIT 2\n",
        "\n",
        "    #Thresholding operation. \n",
        "    #New tensor: 0's where confidence < threshold --> shall be passed to subsequent layers\n",
        "    #1's where confidence > threshold --> shall NOT be passed to subsequent layers.\n",
        "    exiting_instances = tf.cast(tf.where(max_confidence < threshold_ee2, x = 0, y = 1), tf.int32)\n",
        "\n",
        "    #Update output with elements ABOVE threshold that DON'T need to be given to subsequent layers\n",
        "    mask_exiters = tf.equal(exiting_instances, 1) #Mask those elements of batch that took the early exit\n",
        "    output = tf.boolean_mask(ee2_output, mask_exiters) #Take probability vector for the exiter instances\n",
        "\n",
        "    #Identify images that exited, to then sort them\n",
        "    sorting_tensor = tf.math.multiply(auxiliary_tensor,exiting_instances) #Make zero indexes of Batch that didn't exit\n",
        "    mask_non_zeros = tf.not_equal(sorting_tensor, 0) #Take those that aren't zero, so they exited.\n",
        "    sorting_tensor = tf.boolean_mask(sorting_tensor, mask_non_zeros) #Make 1 tensor with id of exited images.\n",
        "\n",
        "    #Update input_non_exiters with elements BELOW threshold that NEED to be given to subsequent layers\n",
        "    mask_non_exiters = tf.equal(exiting_instances, 0) \n",
        "    input_non_exiters = tf.boolean_mask(x_cb3_t, mask_non_exiters) #Take the PIXELS of images that didn't exit here.\n",
        "\n",
        "    #Appending and sortering section\n",
        "    #Array containing vector of confidences for images that took the EE.\n",
        "    output_list.append(output)\n",
        "\n",
        "    #Array containing id and order of the images that took the EE.\n",
        "    sorting_list.append(sorting_tensor)\n",
        "\n",
        "    #Update auxiliary tensor by removing the ids of the images that took the early exit.\n",
        "    auxiliary_tensor = tf.compat.v1.setdiff1d(auxiliary_tensor, sorting_tensor,index_dtype=tf.dtypes.int32)\n",
        "    auxiliary_tensor = auxiliary_tensor[0]\n",
        "\n",
        "  #**********************************FOURTH CONVOLUTIONAL BLOCK***********************************#\n",
        "  '''\n",
        "  Difference must be made. If training, the 4rd CB should take pixels of ALL the images in Batch. \n",
        "  If inference, 4th CB takes pixels of ONLY images that did't take the 1st EE.\n",
        "  '''\n",
        "  if training == True:\n",
        "    input_4cb = x_cb3_t\n",
        "  else: \n",
        "    input_4cb = input_non_exiters\n",
        "\n",
        "  \n",
        "  x_cb4_t = add_conv_block(input_4cb, 192)\n",
        "  #**********************************FOURTH CONVOLUTIONAL BLOCK***********************************#\n",
        "   \n",
        "  #-------------------------------------START FINAL EXIT--------------------------------------#\n",
        "  # Classification done always. Input --> x_cb4_t || Output --> final_output\n",
        "\n",
        "  x = layers.GlobalAvgPool2D()(x_cb4_t) \n",
        "  x = layers.Dense(100, activation='relu')(x)\n",
        "  x = layers.Dropout(0.3)(x)\n",
        "  final_output = layers.Dense(10, activation='softmax')(x)\n",
        "  \n",
        "\n",
        "  final_out = tf.expand_dims(final_output, axis=0)\n",
        "  piu = tf.concat([piu, final_out], axis=0)                       \n",
        "  assert piu.shape == (3, None, 10)\n",
        "  x = piu\n",
        "  \n",
        "  if training == False:\n",
        "    output_list.append(final_output)\n",
        "    sorting_list.append(auxiliary_tensor)\n",
        "\n",
        "    #Sorting operation\n",
        "    sorting_list = tf.concat(sorting_list, axis=0)\n",
        "    sorting_idx = tf.argsort(sorting_list)\n",
        "    output_list = tf.concat(output_list, axis=0)\n",
        "\n",
        "    x = tf.gather(output_list, sorting_idx, batch_dims = 0)\n",
        "\n",
        "  return tf.keras.Model(inputs=inp, outputs=x)"
      ],
      "metadata": {
        "id": "1by7PucitTNO"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_ee = build_model()"
      ],
      "metadata": {
        "id": "zDu6oq_WuJuF"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_ee.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8BHbKLsuMi2",
        "outputId": "7c342943-8aa6-4024-8b15-b02029b42abb"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_4 (InputLayer)           [(None, 500, 375, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " conv2d_12 (Conv2D)             (None, 500, 375, 24  1824        ['input_4[0][0]']                \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_12 (BatchN  (None, 500, 375, 24  96         ['conv2d_12[0][0]']              \n",
            " ormalization)                  )                                                                 \n",
            "                                                                                                  \n",
            " tf.nn.relu_12 (TFOpLambda)     (None, 500, 375, 24  0           ['batch_normalization_12[0][0]'] \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " max_pooling2d_12 (MaxPooling2D  (None, 250, 187, 24  0          ['tf.nn.relu_12[0][0]']          \n",
            " )                              )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_13 (Conv2D)             (None, 250, 187, 48  28848       ['max_pooling2d_12[0][0]']       \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_13 (BatchN  (None, 250, 187, 48  192        ['conv2d_13[0][0]']              \n",
            " ormalization)                  )                                                                 \n",
            "                                                                                                  \n",
            " tf.nn.relu_13 (TFOpLambda)     (None, 250, 187, 48  0           ['batch_normalization_13[0][0]'] \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " max_pooling2d_13 (MaxPooling2D  (None, 125, 93, 48)  0          ['tf.nn.relu_13[0][0]']          \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv2d_14 (Conv2D)             (None, 125, 93, 96)  115296      ['max_pooling2d_13[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_14 (BatchN  (None, 125, 93, 96)  384        ['conv2d_14[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " tf.nn.relu_14 (TFOpLambda)     (None, 125, 93, 96)  0           ['batch_normalization_14[0][0]'] \n",
            "                                                                                                  \n",
            " max_pooling2d_14 (MaxPooling2D  (None, 62, 46, 96)  0           ['tf.nn.relu_14[0][0]']          \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv2d_15 (Conv2D)             (None, 62, 46, 192)  460992      ['max_pooling2d_14[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_15 (BatchN  (None, 62, 46, 192)  768        ['conv2d_15[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " tf.nn.relu_15 (TFOpLambda)     (None, 62, 46, 192)  0           ['batch_normalization_15[0][0]'] \n",
            "                                                                                                  \n",
            " global_average_pooling2d_6 (Gl  (None, 48)          0           ['max_pooling2d_13[0][0]']       \n",
            " obalAveragePooling2D)                                                                            \n",
            "                                                                                                  \n",
            " max_pooling2d_15 (MaxPooling2D  (None, 31, 23, 192)  0          ['tf.nn.relu_15[0][0]']          \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " dense_12 (Dense)               (None, 100)          4900        ['global_average_pooling2d_6[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " global_average_pooling2d_7 (Gl  (None, 96)          0           ['max_pooling2d_14[0][0]']       \n",
            " obalAveragePooling2D)                                                                            \n",
            "                                                                                                  \n",
            " global_average_pooling2d_8 (Gl  (None, 192)         0           ['max_pooling2d_15[0][0]']       \n",
            " obalAveragePooling2D)                                                                            \n",
            "                                                                                                  \n",
            " dropout_6 (Dropout)            (None, 100)          0           ['dense_12[0][0]']               \n",
            "                                                                                                  \n",
            " dense_14 (Dense)               (None, 100)          9700        ['global_average_pooling2d_7[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dense_16 (Dense)               (None, 100)          19300       ['global_average_pooling2d_8[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dense_13 (Dense)               (None, 10)           1010        ['dropout_6[0][0]']              \n",
            "                                                                                                  \n",
            " dropout_7 (Dropout)            (None, 100)          0           ['dense_14[0][0]']               \n",
            "                                                                                                  \n",
            " dropout_8 (Dropout)            (None, 100)          0           ['dense_16[0][0]']               \n",
            "                                                                                                  \n",
            " tf.stack_4 (TFOpLambda)        (None, 10)           0           ['dense_13[0][0]']               \n",
            "                                                                                                  \n",
            " dense_15 (Dense)               (None, 10)           1010        ['dropout_7[0][0]']              \n",
            "                                                                                                  \n",
            " dense_17 (Dense)               (None, 10)           1010        ['dropout_8[0][0]']              \n",
            "                                                                                                  \n",
            " tf.stack_5 (TFOpLambda)        (2, None, 10)        0           ['tf.stack_4[0][0]',             \n",
            "                                                                  'dense_15[0][0]']               \n",
            "                                                                                                  \n",
            " tf.expand_dims_1 (TFOpLambda)  (1, None, 10)        0           ['dense_17[0][0]']               \n",
            "                                                                                                  \n",
            " tf.concat_1 (TFOpLambda)       (3, None, 10)        0           ['tf.stack_5[0][0]',             \n",
            "                                                                  'tf.expand_dims_1[0][0]']       \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 645,330\n",
            "Trainable params: 644,610\n",
            "Non-trainable params: 720\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_ee.compile(loss=early_exit_loss, optimizer=optimizer, metrics=[accuracy])"
      ],
      "metadata": {
        "id": "sEQXNhc7uW1O"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_ee.fit(train_data_p, validation_data=val_data_p, epochs=1, callbacks=cbs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNlXO2X3ulwi",
        "outputId": "27fc9a00-f5f9-49ff-abe4-8be11d6477ae"
      },
      "execution_count": 47,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "67/67 [==============================] - 824s 12s/step - loss: 8.8554 - sparse_categorical_accuracy: 0.1157 - val_loss: 8.1512 - val_sparse_categorical_accuracy: 0.1134\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe3e9168e50>"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sketching a new way..."
      ],
      "metadata": {
        "id": "iECDqYxgExox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(training = True):\n",
        "  #*****INPUT******\n",
        "  inp = layers.Input(shape=(500, 375, 3))\n",
        "  #*****FIRST CONVOLUTIONAL BLOCK******\n",
        "\n",
        "  if training == True:\n",
        "    x_cb1_t = add_conv_block(inp, 6)\n",
        "  else:\n",
        "    x_cb1 = add_conv_block(inp,6)\n",
        "\n",
        "  \n",
        "\n",
        "  #*****FIRST EARLY EXIT******\n",
        "  if training == True:\n",
        "     \n",
        "    x = layers.GlobalAvgPool2D()(x_cb1_t) #Possibly have to change names\n",
        "    x = layers.Dense(100, activation='relu')(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    ee1_output = layers.Dense(10, activation='softmax')(x)\n",
        "\n",
        "    piu = tf.stack(ee1_output, axis=0) \n",
        "    \n",
        "\n",
        "  else: \n",
        "    x = layers.GlobalAvgPool2D()(x_cb1) \n",
        "    x = layers.Dense(100, activation='relu')(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    output = layers.Dense(10, activation='softmax')(x) \n",
        "\n",
        "    threshold_ee1 = 0.9 \n",
        "    batch_size = 12\n",
        "    auxiliary_tensor = tf.range(batch_size)\n",
        "    auxiliary_tensor = tf.reshape(auxiliary_tensor, [-1])\n",
        "    \n",
        "    #Take for each image of the Batch the category with highest confidence after the softmax.\n",
        "    max_confidence = tf.reduce_max(output, axis = -1)\n",
        "\n",
        "    #Thresholding operation. \n",
        "    #New tensor with 0's where confidence is below threshold --> shall be passed to subsequent layers\n",
        "    #1's where confidence is above threshold --> shall NOT be passed to subsequent layers.\n",
        "    exiting_instances = tf.cast(tf.where(max_confidence < threshold_ee1, 0, 1), tf.int32)\n",
        "\n",
        "    #Update output with elements ABOVE threshold that DON'T need to be given to subsequent layers\n",
        "    mask_exiters = tf.equal(exiting_instances, 1) #Mask those elements of batch that took the early exit\n",
        "    output = tf.boolean_mask(output, mask_exiters) #Take probability vector for the exiter instances\n",
        "\n",
        "    sorting_tensor = tf.math.multiply(auxiliary_tensor,exiting_instances) #Make zero indexes of Batch that didn't exit\n",
        "    mask_non_zeros = tf.not_equal(sorting_tensor, 0) #Take indexes that aren't zero\n",
        "    sorting_tensor = tf.boolean_mask(sorting_tensor, mask_non_zeros) #Tensor to sort the output according to instance to which they belonged\n",
        "\n",
        "    #Update input_non_exiters with elements BELOW threshold that NEED to be given to subsequent layers\n",
        "    mask_non_exiters = tf.equal(exiting_instances, 0)\n",
        "    input_non_exiters = tf.boolean_mask(x_cb1, mask_non_exiters) #多?多?In testing, I update which members of the Batch didn't take EE --> must be passed\n",
        "\n",
        "\n",
        "    output_list = []\n",
        "    output_list.append(output)\n",
        "\n",
        "    sorting_list = []\n",
        "    sorting_list.append(sorting_tensor)\n",
        "\n",
        "    auxiliary_tensor = tf.compat.v1.setdiff1d(auxiliary_tensor, sorting_tensor,index_dtype=tf.dtypes.int32)\n",
        "    auxiliary_tensor = auxiliary_tensor[0]\n",
        "    #*****FIRST EARLY EXIT******\n",
        "\n",
        "  \n",
        "  #*****SECOND CONVOLUTIONAL BLOCK******\n",
        "  if training == True:\n",
        "    x_cb2_t = add_conv_block(x_cb1_t, 6)\n",
        "  else:\n",
        "    x_cb2 = add_conv_block(input_non_exiters,6)\n",
        "\n",
        "  #*****SECOND EARLY EXIT******\n",
        "  if training == True:\n",
        "     \n",
        "    x = layers.GlobalAvgPool2D()(x_cb2_t) #Possibly have to change names\n",
        "    x = layers.Dense(100, activation='relu')(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    ee2_output = layers.Dense(10, activation='softmax')(x) \n",
        "\n",
        "    piu = tf.stack([piu,ee2_output], axis=0)                                                             \n",
        "    \n",
        "\n",
        "  else: \n",
        "    x = layers.GlobalAvgPool2D()(x_cb2) \n",
        "    x = layers.Dense(100, activation='relu')(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    output = layers.Dense(10, activation='softmax')(x) \n",
        "\n",
        "    threshold_ee2 = 0.8 #define threshold?\n",
        "  \n",
        "    \n",
        "    #Take for each image of the Batch the category with highest confidence after the softmax.\n",
        "    max_confidence = tf.reduce_max(output, axis = -1)\n",
        "\n",
        "    #Thresholding operation. \n",
        "    #New tensor with 0's where confidence is below threshold --> shall be passed to subsequent layers\n",
        "    #1's where confidence is above threshold --> shall NOT be passed to subsequent layers.\n",
        "    exiting_instances = tf.cast(tf.where(max_confidence < threshold_ee2, x = 0, y = 1), tf.int32)\n",
        "\n",
        "    #Update output with elements ABOVE threshold that DON'T need to be given to subsequent layers\n",
        "    mask_exiters = tf.equal(exiting_instances, 1) #Mask those elements of batch that took the early exit\n",
        "    output = tf.boolean_mask(output, mask_exiters) #Take probability vector for the exiter instances\n",
        "\n",
        "    sorting_tensor = tf.math.multiply(auxiliary_tensor,exiting_instances) #Make zero indexes of Batch that didn't exit\n",
        "    mask_non_zeros = tf.not_equal(sorting_tensor, 0) #Take indexes that aren't zero\n",
        "    sorting_tensor = tf.boolean_mask(sorting_tensor, mask_non_zeros) #Tensor to sort the output according to instance to which they belonged\n",
        "\n",
        "    #Update input_non_exiters with elements BELOW threshold that NEED to be given to subsequent layers\n",
        "    mask_non_exiters = tf.equal(exiting_instances, 0)\n",
        "    input_non_exiters = tf.boolean_mask(x_cb2, mask_non_exiters) #多?多?In testing, I update which members of the Batch didn't take EE --> must be passed\n",
        "\n",
        "    #Appending operation\n",
        "    output_list.append(output)\n",
        "    sorting_list.append(sorting_tensor)\n",
        "\n",
        "    #Auxiliary tensor update --> possibly dispensable\n",
        "    auxiliary_tensor = tf.compat.v1.setdiff1d(auxiliary_tensor, sorting_tensor,index_dtype=tf.dtypes.int32)\n",
        "    auxiliary_tensor = auxiliary_tensor[0]\n",
        "\n",
        "    #*****SECOND EARLY EXIT******\n",
        "\n",
        "    #*****FINAL EXIT******\n",
        "  if training == True:\n",
        "    x = layers.GlobalAvgPool2D()(x_cb2_t) \n",
        "    x = layers.Dense(100, activation='relu')(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    final_output = layers.Dense(10, activation='softmax')(x)\n",
        "    \n",
        "    final_output = tf.expand_dims(final_output, axis=0)\n",
        "    piu = tf.concat([piu, final_output], axis=0)\n",
        "    #piu = tf.stack([piu,final_output], axis=0)                         \n",
        "    assert piu.shape == (3, None, 10)\n",
        "    x = piu\n",
        "\n",
        "  else:\n",
        "    output = layers.GlobalAvgPool2D()(input_non_exiters) \n",
        "    output = layers.Dense(100, activation='relu')(output)\n",
        "    output = layers.Dropout(0.3)(output)\n",
        "    output = layers.Dense(10, activation='softmax')(output)    \n",
        "\n",
        "    #Appending Operation\n",
        "    output_list.append(output)\n",
        "    sorting_list.append(sorting_tensor)\n",
        "    \n",
        "    #Sorting operation\n",
        "    sorting_list = tf.concat(sorting_list, axis=0)\n",
        "    sorting_idx = tf.argsort(sorting_list)\n",
        "    output_list = tf.concat(output_list, axis=0)\n",
        "\n",
        "    x = tf.gather(output_list, sorting_idx, batch_dims = 0)\n",
        "\n",
        "\n",
        "  return tf.keras.Model(inputs=inp, outputs=x)"
      ],
      "metadata": {
        "id": "4_pKbnphEbVm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomModel(keras.Model):\n",
        "    \n",
        "    \n",
        "    def build_model(training = None):\n",
        "      #*****INPUT******\n",
        "      inp = layers.Input(shape=(500, 375, 3))\n",
        "      #*****FIRST CONVOLUTIONAL BLOCK******\n",
        "\n",
        "      if training == True:\n",
        "        x_cb1_t = add_conv_block(inp, 6)\n",
        "      else:\n",
        "        x_cb1 = add_conv_block(inp,6)\n",
        "\n",
        "      #*****FIRST EARLY EXIT******\n",
        "      if training == True:\n",
        "     \n",
        "        x = layers.GlobalAvgPool2D()(x_cb1_t) #Possibly have to change names\n",
        "        x = layers.Dense(100, activation='relu')(x)\n",
        "        x = layers.Dropout(0.3)(x)\n",
        "        ee1_output = layers.Dense(10, activation='softmax')(x)\n",
        "\n",
        "        piu = tf.stack(ee1_output, axis=0) \n",
        "        print(piu)\n",
        "\n",
        "      else: \n",
        "        x = layers.GlobalAvgPool2D()(x_cb1) \n",
        "        x = layers.Dense(100, activation='relu')(x)\n",
        "        x = layers.Dropout(0.3)(x)\n",
        "        output = layers.Dense(10, activation='softmax')(x) \n",
        "\n",
        "        threshold_ee1 = 0.9 \n",
        "        batch_size = 12\n",
        "        auxiliary_tensor = tf.range(batch_size)\n",
        "        auxiliary_tensor = tf.reshape(auxiliary_tensor, [-1])\n",
        "    \n",
        "        #Take for each image of the Batch the category with highest confidence after the softmax.\n",
        "        max_confidence = tf.reduce_max(output, axis = -1)\n",
        "\n",
        "        #Thresholding operation. \n",
        "        #New tensor with 0's where confidence is below threshold --> shall be passed to subsequent layers\n",
        "        #1's where confidence is above threshold --> shall NOT be passed to subsequent layers.\n",
        "        exiting_instances = tf.cast(tf.where(max_confidence < threshold_ee1, 0, 1), tf.int32)\n",
        "\n",
        "        #Update output with elements ABOVE threshold that DON'T need to be given to subsequent layers\n",
        "        mask_exiters = tf.equal(exiting_instances, 1) #Mask those elements of batch that took the early exit\n",
        "        output = tf.boolean_mask(output, mask_exiters) #Take probability vector for the exiter instances\n",
        "\n",
        "        sorting_tensor = tf.math.multiply(auxiliary_tensor,exiting_instances) #Make zero indexes of Batch that didn't exit\n",
        "        mask_non_zeros = tf.not_equal(sorting_tensor, 0) #Take indexes that aren't zero\n",
        "        sorting_tensor = tf.boolean_mask(sorting_tensor, mask_non_zeros) #Tensor to sort the output according to instance to which they belonged\n",
        "\n",
        "        #Update input_non_exiters with elements BELOW threshold that NEED to be given to subsequent layers\n",
        "        mask_non_exiters = tf.equal(exiting_instances, 0)\n",
        "        input_non_exiters = tf.boolean_mask(x_cb1, mask_non_exiters) #多?多?In testing, I update which members of the Batch didn't take EE --> must be passed\n",
        "\n",
        "\n",
        "        output_list = []\n",
        "        output_list.append(output)\n",
        "\n",
        "        sorting_list = []\n",
        "        sorting_list.append(sorting_tensor)\n",
        "\n",
        "        auxiliary_tensor = tf.compat.v1.setdiff1d(auxiliary_tensor, sorting_tensor,index_dtype=tf.dtypes.int32)\n",
        "        auxiliary_tensor = auxiliary_tensor[0]\n",
        "        #*****FIRST EARLY EXIT******\n",
        "\n",
        "  \n",
        "      #*****SECOND CONVOLUTIONAL BLOCK******\n",
        "      if training == True:\n",
        "        x_cb2_t = add_conv_block(x_cb1_t, 6)\n",
        "      else:\n",
        "        x_cb2 = add_conv_block(input_non_exiters,6)\n",
        "\n",
        "      #*****SECOND EARLY EXIT******\n",
        "      if training == True:\n",
        "     \n",
        "        x = layers.GlobalAvgPool2D()(x_cb2_t) #Possibly have to change names\n",
        "        x = layers.Dense(100, activation='relu')(x)\n",
        "        x = layers.Dropout(0.3)(x)\n",
        "        ee2_output = layers.Dense(10, activation='softmax')(x) \n",
        "\n",
        "        piu = tf.stack([piu,ee2_output], axis=0)                                                             \n",
        "        print(f\"Piu EE2: {piu}\")\n",
        "\n",
        "      else: \n",
        "        x = layers.GlobalAvgPool2D()(x_cb2) \n",
        "        x = layers.Dense(100, activation='relu')(x)\n",
        "        x = layers.Dropout(0.3)(x)\n",
        "        output = layers.Dense(10, activation='softmax')(x) \n",
        "\n",
        "        threshold_ee2 = 0.8 #define threshold?\n",
        "  \n",
        "    \n",
        "        #Take for each image of the Batch the category with highest confidence after the softmax.\n",
        "        max_confidence = tf.reduce_max(output, axis = -1)\n",
        "\n",
        "        #Thresholding operation. \n",
        "        #New tensor with 0's where confidence is below threshold --> shall be passed to subsequent layers\n",
        "        #1's where confidence is above threshold --> shall NOT be passed to subsequent layers.\n",
        "        exiting_instances = tf.cast(tf.where(max_confidence < threshold_ee2, x = 0, y = 1), tf.int32)\n",
        "\n",
        "        #Update output with elements ABOVE threshold that DON'T need to be given to subsequent layers\n",
        "        mask_exiters = tf.equal(exiting_instances, 1) #Mask those elements of batch that took the early exit\n",
        "        output = tf.boolean_mask(output, mask_exiters) #Take probability vector for the exiter instances\n",
        "\n",
        "        sorting_tensor = tf.math.multiply(auxiliary_tensor,exiting_instances) #Make zero indexes of Batch that didn't exit\n",
        "        mask_non_zeros = tf.not_equal(sorting_tensor, 0) #Take indexes that aren't zero\n",
        "        sorting_tensor = tf.boolean_mask(sorting_tensor, mask_non_zeros) #Tensor to sort the output according to instance to which they belonged\n",
        "\n",
        "        #Update input_non_exiters with elements BELOW threshold that NEED to be given to subsequent layers\n",
        "        mask_non_exiters = tf.equal(exiting_instances, 0)\n",
        "        input_non_exiters = tf.boolean_mask(x_cb2, mask_non_exiters) #多?多?In testing, I update which members of the Batch didn't take EE --> must be passed\n",
        "\n",
        "        #Appending operation\n",
        "        output_list.append(output)\n",
        "        print(f\"Early Exit 1: {output_list}\")\n",
        "        sorting_list.append(sorting_tensor)\n",
        "\n",
        "        #Auxiliary tensor update --> possibly dispensable\n",
        "        auxiliary_tensor = tf.compat.v1.setdiff1d(auxiliary_tensor, sorting_tensor,index_dtype=tf.dtypes.int32)\n",
        "        auxiliary_tensor = auxiliary_tensor[0]\n",
        "\n",
        "        #*****SECOND EARLY EXIT******\n",
        "\n",
        "        #*****FINAL EXIT******\n",
        "      if training == True:\n",
        "        x = layers.GlobalAvgPool2D()(x_cb2_t) \n",
        "        x = layers.Dense(100, activation='relu')(x)\n",
        "        x = layers.Dropout(0.3)(x)\n",
        "        final_output = layers.Dense(10, activation='softmax')(x)\n",
        "    \n",
        "        final_output = tf.expand_dims(final_output, axis=0)\n",
        "        piu = tf.concat([piu, final_output], axis=0)\n",
        "        #piu = tf.stack([piu,final_output], axis=0)                         \n",
        "        assert piu.shape == (3, None, 10)\n",
        "        x = piu\n",
        "\n",
        "      else:\n",
        "        output = layers.GlobalAvgPool2D()(input_non_exiters) \n",
        "        output = layers.Dense(100, activation='relu')(output)\n",
        "        output = layers.Dropout(0.3)(output)\n",
        "        output = layers.Dense(10, activation='softmax')(output)    \n",
        "\n",
        "        #Appending Operation\n",
        "        output_list.append(output)\n",
        "        sorting_list.append(sorting_tensor)\n",
        "    \n",
        "        #Sorting operation\n",
        "        sorting_list = tf.concat(sorting_list, axis=0)\n",
        "        sorting_idx = tf.argsort(sorting_list)\n",
        "        output_list = tf.concat(output_list, axis=0)\n",
        "\n",
        "        x = tf.gather(output_list, sorting_idx, batch_dims = 0)\n",
        "\n",
        "\n",
        "      return tf.keras.Model(inputs=inp, outputs=x)\n",
        "    \n",
        "    def train_step(self, data):\n",
        "        # Unpack the data. Its structure depends on your model and\n",
        "        # on what you pass to `fit()`.\n",
        "        x, y = data\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_pred = self(x, training=True)  # Forward pass\n",
        "            # Compute the loss value\n",
        "            # (the loss function is configured in `compile()`)\n",
        "            loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n",
        "\n",
        "        # Compute gradients\n",
        "        trainable_vars = self.trainable_variables\n",
        "        gradients = tape.gradient(loss, trainable_vars)\n",
        "        # Update weights\n",
        "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "        # Update metrics (includes the metric that tracks the loss)\n",
        "        self.compiled_metrics.update_state(y, y_pred)\n",
        "        # Return a dict mapping metric names to current value\n",
        "        return {m.name: m.result() for m in self.metrics}"
      ],
      "metadata": {
        "id": "YfNTlVr_g11o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CustomModel.build_model()"
      ],
      "metadata": {
        "id": "0HS46plfB-27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "sNwHr-6WCa8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def early_exit_loss(y_true,y_preds):\n",
        "  scce = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "  loss_ee1 = scce(y_true, y_preds[0])\n",
        "  loss_ee2 = scce(y_true, y_preds[1])\n",
        "  loss_final = scce(y_true, y_preds[2])\n",
        "\n",
        "  return loss_final + loss_ee1*1 + loss_ee2*1"
      ],
      "metadata": {
        "id": "TxdNrygKDMql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cross_entropy = losses.SparseCategoricalCrossentropy(from_logits=True) \n",
        "accuracy = metrics.SparseCategoricalAccuracy()\n",
        "optimizer = optimizers.Adam()"
      ],
      "metadata": {
        "id": "YIVW8OJuC1cZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=early_exit_loss, optimizer=optimizer, metrics=[accuracy])"
      ],
      "metadata": {
        "id": "sFSC7JAWC8VI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=early_exit_loss, optimizer=optimizer, metrics=[accuracy])\n",
        "model.fit(train_data_p, validation_data=val_data_p, epochs=1, callbacks=cbs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "id": "PAZ91lI9JWIE",
        "outputId": "a4af97b7-3499-49d6-84a9-3b8e5d5508c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/model_3/tf.concat_2/sub_2:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/model_3/tf.concat_2/GatherV2_8:0\", shape=(None, 10), dtype=float32), dense_shape=Tensor(\"gradient_tape/model_3/tf.concat_2/Shape_2:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-b7089fe64cfb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_data_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1407\u001b[0m                 _r=1):\n\u001b[1;32m   1408\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1410\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    978\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 980\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    981\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    982\u001b[0m       _, _, filtered_flat_args = (\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2450\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2451\u001b[0m       (graph_function,\n\u001b[0;32m-> 2452\u001b[0;31m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0m\u001b[1;32m   2453\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m   2454\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2709\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache_key\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_placeholder_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2711\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2712\u001b[0m           self._function_cache.add(cache_key, cache_key_deletion_observer,\n\u001b[1;32m   2713\u001b[0m                                    graph_function)\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2625\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2626\u001b[0m     graph_function = ConcreteFunction(\n\u001b[0;32m-> 2627\u001b[0;31m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m   2628\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2629\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, acd_record_initial_resource_uses)\u001b[0m\n\u001b[1;32m   1139\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1141\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    675\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1114\u001b[0m           \u001b[0;31m# TODO(mdan): Push this block higher in tf.function's call stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m           \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1116\u001b[0;31m             return autograph.converted_call(\n\u001b[0m\u001b[1;32m   1117\u001b[0m                 \u001b[0moriginal_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    437\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverted_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0meffective_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverted_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0meffective_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    329\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mconversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_in_allowlist_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Allowlisted %s: from cache'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_call_unconverted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_status_ctx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDISABLED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    457\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mstep_function\u001b[0;34m(model, iterator)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             run_step, jit_compile=True, reduce_retracing=True)\n\u001b[1;32m   1039\u001b[0m       \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute_strategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m       outputs = reduce_per_replica(\n\u001b[1;32m   1042\u001b[0m           outputs, self.distribute_strategy, reduction='first')\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1310\u001b[0m       fn = autograph.tf_convert(\n\u001b[1;32m   1311\u001b[0m           fn, autograph_ctx.control_status_ctx(), convert_by_default=False)\n\u001b[0;32m-> 1312\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extended\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mcall_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   2886\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2887\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2888\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2889\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2890\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36m_call_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   3687\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3688\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mReplicaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplica_id_in_sync_group\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3689\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3691\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_reduce_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestinations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    687\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconversion_ctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 689\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    690\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_requested\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_allowlisted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_call_unconverted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m   \u001b[0;31m# internal_convert_user_code is for example turned off when issuing a dynamic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mrun_step\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1029\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0mrun_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1030\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1031\u001b[0m         \u001b[0;31m# Ensure counter is updated only if `train_step` succeeds.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_minimum_control_deps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    891\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_target_and_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m     \u001b[0;31m# Run backwards pass.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 893\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    894\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(self, loss, var_list, grad_loss, name, tape)\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m     \"\"\"\n\u001b[0;32m--> 537\u001b[0;31m     grads_and_vars = self._compute_gradients(\n\u001b[0m\u001b[1;32m    538\u001b[0m         loss, var_list=var_list, grad_loss=grad_loss, tape=tape)\n\u001b[1;32m    539\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36m_compute_gradients\u001b[0;34m(self, loss, var_list, grad_loss, tape)\u001b[0m\n\u001b[1;32m    588\u001b[0m     \u001b[0mvar_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/gradients\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m       \u001b[0mgrads_and_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m     self._assert_valid_dtypes([\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36m_get_gradients\u001b[0;34m(self, tape, loss, var_list, grad_loss)\u001b[0m\n\u001b[1;32m    469\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_get_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[0;34m\"\"\"Called in `minimize` to compute gradients from loss.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m     \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                           for x in output_gradients]\n\u001b[1;32m   1099\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m     flat_grad = imperative_grad.imperative_grad(\n\u001b[0m\u001b[1;32m   1101\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m         \u001b[0mflat_targets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \"Unknown value for unconnected_gradients: %r\" % unconnected_gradients)\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m   return pywrap_tfe.TFE_Py_TapeGradient(\n\u001b[0m\u001b[1;32m     68\u001b[0m       \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/array_grad.py\u001b[0m in \u001b[0;36m_ReshapeGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_ReshapeGrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m   return [\n\u001b[0;32m--> 791\u001b[0;31m       array_ops.reshape(\n\u001b[0m\u001b[1;32m    792\u001b[0m           _IndexedSlicesToTensorNoWarning(grad), array_ops.shape(op.inputs[0])),\n\u001b[1;32m    793\u001b[0m       \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1080\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(tensor, shape, name)\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mHas\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mas\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m   \"\"\"\n\u001b[0;32m--> 202\u001b[0;31m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m   \u001b[0mtensor_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_set_static_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(tensor, shape, name)\u001b[0m\n\u001b[1;32m   8544\u001b[0m       \u001b[0;32mpass\u001b[0m  \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8545\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 8546\u001b[0;31m   _, _, _op, _outputs = _op_def_library._apply_op_helper(\n\u001b[0m\u001b[1;32m   8547\u001b[0m         \"Reshape\", tensor=tensor, shape=shape, name=name)\n\u001b[1;32m   8548\u001b[0m   \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    795\u001b[0m       \u001b[0;31m# Add Op to graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m       \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 797\u001b[0;31m       op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n\u001b[0m\u001b[1;32m    798\u001b[0m                                  \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m                                  attrs=attr_protos, op_def=op_def)\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m    692\u001b[0m       \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcapture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m       \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 694\u001b[0;31m     return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    695\u001b[0m         \u001b[0mop_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m         compute_device)\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m   3752\u001b[0m     \u001b[0;31m# Session.run call cannot occur between creating and mutating the op.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3753\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mutation_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3754\u001b[0;31m       ret = Operation(\n\u001b[0m\u001b[1;32m   3755\u001b[0m           \u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3756\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[1;32m   2127\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mop_def\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2128\u001b[0m         \u001b[0mop_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_op_def\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2129\u001b[0;31m       self._c_op = _create_c_op(self._graph, node_def, inputs,\n\u001b[0m\u001b[1;32m   2130\u001b[0m                                 control_input_ops, op_def)\n\u001b[1;32m   2131\u001b[0m       \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs, op_def)\u001b[0m\n\u001b[1;32m   1958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1959\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1960\u001b[0;31m     \u001b[0mc_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpywrap_tf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1961\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1962\u001b[0m     \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiments on tensor indexing"
      ],
      "metadata": {
        "id": "5PG_RSGrswLF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUMasSh8_5Ht"
      },
      "source": [
        "#### Testing the Boolean Mask trick - Retriving **Exiters** members of Batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pHXg4mFPz05d"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "example_tensor = tf.constant([[0.33, 0.33, 0.34],\n",
        "                             [0.9, 0.05, 0.05],\n",
        "                             [0.2, 0.6, 0.2]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-bPqYqW14pq"
      },
      "outputs": [],
      "source": [
        "max_confidence = tf.reduce_max(example_tensor, axis = -1)\n",
        "max_confidence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gfFDsozC14wG"
      },
      "outputs": [],
      "source": [
        "exiting_elements = tf.cast(tf.where(max_confidence < 0.8, x=0, y=1), tf.int32)\n",
        "#exiting_elements = tf.reshape(exiting_elements, [1, -1])\n",
        "exiting_elements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fmgN3Rusfc6F"
      },
      "outputs": [],
      "source": [
        "ind_tensor = tf.cast(tf.constant([0,1,0]),tf.int32)\n",
        "ind_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ntVBaqQfgBJ"
      },
      "outputs": [],
      "source": [
        "mask = tf.equal(exiting_elements, 1)\n",
        "result = tf.boolean_mask(example_tensor, mask)\n",
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvPbILO5BjKx"
      },
      "source": [
        "#### Testing the Boolean Mask trick - Retriving **Non Exiters** members of Batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xCkjZ0qVfJ6"
      },
      "outputs": [],
      "source": [
        "for xb, yb in train_data_p:\n",
        "  first_element_batch = xb\n",
        "  second_element_batch = yb\n",
        "  print(xb.shape)\n",
        "  print(yb.shape)\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2M4k8RyDB9ep"
      },
      "outputs": [],
      "source": [
        "first_element_batch.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4aHcIOERvCsM"
      },
      "outputs": [],
      "source": [
        "ind_tensor = tf.cast(tf.constant([1,0,1,0,0,0,0,0,0,0,0,0]),tf.int32)\n",
        "ind_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xMQlOaRSCGcE"
      },
      "outputs": [],
      "source": [
        "mask = tf.equal(ind_tensor, 0)\n",
        "result = tf.boolean_mask(first_element_batch, mask)\n",
        "result.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KXGrKzB8HMhf"
      },
      "outputs": [],
      "source": [
        "index_tensor = tf.constant([0, 2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1tYo_L2_Nyk"
      },
      "source": [
        "#### Testing the function to create the **sortering_tensor**\n",
        "Why do we need to reorder the output? Because we're gonna give to the EarlyExit layer only the X (pixels of image) and not the Y (true label). Then, during inference we need to see which images exit in which early exit to compute the costs but, since the images won't exit in an ordered way and the order of the true labels (y) is static, we need to reorder the vector of class probabilities such that they match their corresponding true label. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ZsMZS7JCADT"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "example_tensor = tf.constant([[0.33, 0.33, 0.34],\n",
        "                             [0.9, 0.05, 0.05],\n",
        "                             [0.2, 0.6, 0.2],\n",
        "                              [0.85, 0.05, 0.1]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v2x-sBbaCAJ5"
      },
      "outputs": [],
      "source": [
        "max_confidence = tf.reduce_max(example_tensor, axis = -1)\n",
        "max_confidence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cwdnaY0cBpkm"
      },
      "outputs": [],
      "source": [
        "exiting_elements = tf.cast(tf.where(max_confidence < 0.8, x=0, y=1), tf.int32)\n",
        "exiting_elements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMR9IH00Bpor"
      },
      "outputs": [],
      "source": [
        "mask = tf.equal(exiting_elements, 1)\n",
        "result = tf.boolean_mask(example_tensor, mask)\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PNkbe66uBpur"
      },
      "outputs": [],
      "source": [
        "input_index = tf.range(example_tensor.shape[0])\n",
        "input_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ESaucm9OBpx_"
      },
      "outputs": [],
      "source": [
        "exit_order = tf.math.multiply(input_index,exiting_elements)\n",
        "exit_order"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r4JbLlJNI4hs"
      },
      "outputs": [],
      "source": [
        "mask_non_zeros = tf.not_equal(exit_order, 0)\n",
        "mask_non_zeros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TXPNvMkJI4pK"
      },
      "outputs": [],
      "source": [
        "exit_order = tf.boolean_mask(exit_order, mask_non_zeros)\n",
        "exit_order"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psmbn-VWi9wI"
      },
      "source": [
        "#### **Discovering how to the update the Auxiliary tensor**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OVxmKYrEi8pI"
      },
      "outputs": [],
      "source": [
        "''''\n",
        "Create ficticious tensor that represents the auxiliary tensor \n",
        "'''\n",
        "import tensorflow as tf\n",
        "auxiliary_tensor = tf.constant([[0,1,2,3,4,5,6,7,8,9,10,11]])\n",
        "auxiliary_tensor = tf.reshape(auxiliary_tensor, [-1])\n",
        "auxiliary_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NyR-oTC9j4gt"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Recreate situation in which image 3 and 7 have taken the EarlyExit\n",
        "'''\n",
        "sorting_tensor = tf.constant([[3,7]])\n",
        "sorting_tensor = tf.reshape(sorting_tensor, [-1])\n",
        "sorting_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_RKqQVgj9GB"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Out of the EarlyExit layer, we remove from the Auxiliary Tensor, those elements that took the early exit.\n",
        "This is done since we want to feed this new auxiliary_tensor to the 2nd EarlyExit Layer, which mustn't\n",
        "contain the elements that exited on previous EarlyExit. Images must be identified with the same index\n",
        "regardless of where they exit.\n",
        "\n",
        "See that auxiliary_tensor doesn't contain the exited images anymore. \n",
        "'''\n",
        "auxiliary_tensor = tf.compat.v1.setdiff1d(auxiliary_tensor, sorting_tensor,index_dtype=tf.dtypes.int32)\n",
        "auxiliary_tensor = auxiliary_tensor[0]\n",
        "auxiliary_tensor"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#For the difference with the auxiliary tensor during training.\n",
        "'''\n",
        "However, see that the EarlyExit layer outputs always 3 elements:\n",
        "1) outputs: vector of probabilities.\n",
        "2) sorting tensor: used to keep track of which images took the early exit\n",
        "3) input_non_exiters: tensor of pixels of images that didn't take the EarlyExit\n",
        "and we need to give to subsequent Layers \n",
        "\n",
        "During training and inference, they have different behaviors. \n",
        "1.1 Outputs - Training: contains vector of probabilities for all the instances, since all took the EarlyExit\n",
        "1.2 Outputs - Inference: contains vector of probabilities of ONLY those who took the EarlyExit and EXITED. \n",
        "\n",
        "2.1 Sorting_tensor - Training: empty tensor bc we don't need to update the auxiliary_tensor \n",
        "(operation done out of the EarlyExit Layer) bc all instances took the early exit, therefore they're sorted. \n",
        "2.2 Sorting_tensor - Inference: it identifies the images that took the EarlyExit. We need this tensor\n",
        "to update, out of the EarlyExit Layer, the auxiliary tensor that will keep track of which images will\n",
        "enter the next EarlyExit. \n",
        "\n",
        "3.3 Input_non_exiters - Training: contain all the pixels, bc we need to pass all images to subsequent layers.\n",
        "3.4 Input_non_exiters - Testing: contain pixels of only those images that didn't take the early exit. \n",
        "\n",
        "So, this empty tensor will be the sorting_tensor DURING TRAINING. This way, during training, the sorting_tensor output\n",
        "has one single element -1 (tf.zeros or tf.ones no bc there are 0 and 1 indexes, but there's no index or element -1), \n",
        "such that the update operation of the auxiliary_tensor (done out of the earlyExitLayer) doesn't remove any element of \n",
        "the tensor. This last because during training all images enter the EarlyExit.\n",
        "'''\n",
        "empty_tensor = tf.constant([[-1]], dtype=tf.int32)\n",
        "empty_tensor = tf.reshape(empty_tensor, [-1])\n",
        "empty_tensor\n",
        "#empty_tensor = tf.experimental.numpy.empty([1, 0], dtype=tf.float32)\n",
        "#empty_tensor"
      ],
      "metadata": {
        "id": "6DZT7cz9ERqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Operation done out of the EarlyExit layer. It updates the auxiliary_tensor such that\n",
        "it contains the identifiers of the images that will enter the next EarlyExit. \n",
        "'''\n",
        "#Testing the difference\n",
        "auxiliary_tensor = tf.compat.v1.setdiff1d(auxiliary_tensor, empty_tensor,index_dtype=tf.dtypes.int32)\n",
        "auxiliary_tensor"
      ],
      "metadata": {
        "id": "BgJjQ30qFwkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6Y6fdtyW2vp"
      },
      "source": [
        "#### **Recreating the sortering...during INFERENCE**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "output = tf.constant([[0.33, 0.33, 0.34],\n",
        "                             [0.9, 0.05, 0.05],\n",
        "                             [0.2, 0.6, 0.2],\n",
        "                              [0.85, 0.05, 0.1]])"
      ],
      "metadata": {
        "id": "j4lSLYLXQwhH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "auxiliary_tensor = tf.range(output.shape[0])\n",
        "auxiliary_tensor = tf.reshape(auxiliary_tensor, [-1])\n",
        "auxiliary_tensor"
      ],
      "metadata": {
        "id": "QTAnNZQggugp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_confidence = tf.reduce_max(output, axis = -1) #Highest probs\n",
        "exiting_instances = tf.cast(tf.where(max_confidence < 0.8, x=0, y=1), tf.int32) #Thresholding. 1 and 0 tensor\n",
        "mask_exiters = tf.equal(exiting_instances, 1) #Mask. Extract only those who exited\n",
        "output = tf.boolean_mask(output, mask_exiters) \n",
        "output"
      ],
      "metadata": {
        "id": "g_zVNQuAQwks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorting_tensor = tf.math.multiply(auxiliary_tensor,exiting_instances) #Make zero indexes of Batch that didn't exit\n",
        "mask_non_zeros = tf.not_equal(sorting_tensor, 0) #Take indexes that aren't zero\n",
        "sorting_tensor = tf.boolean_mask(sorting_tensor, mask_non_zeros) #Tensor to sort the output according to instance to which they belonged\n",
        "sorting_tensor"
      ],
      "metadata": {
        "id": "ufkyh-dmhj1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_list = []\n",
        "output_list.append(output)"
      ],
      "metadata": {
        "id": "6miSvboUo4Xy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorting_list = []\n",
        "sorting_list.append(sorting_tensor)"
      ],
      "metadata": {
        "id": "hENuseAhnJ1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "auxiliary_tensor = tf.compat.v1.setdiff1d(auxiliary_tensor, sorting_tensor,index_dtype=tf.dtypes.int32)\n",
        "auxiliary_tensor = auxiliary_tensor[0]\n",
        "auxiliary_tensor"
      ],
      "metadata": {
        "id": "y_z1QSEciL2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Second Layer...**"
      ],
      "metadata": {
        "id": "N2Gc6C1clG9h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = tf.constant([[0.33, 0.33, 0.34],\n",
        "                      [0.2, 0.6, 0.2]])"
      ],
      "metadata": {
        "id": "lvUEZEvJlGc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_confidence = tf.reduce_max(output, axis = -1) #Highest probs\n",
        "exiting_instances = tf.cast(tf.where(max_confidence < 0.6, x=0, y=1), tf.int32) #Thresholding. 1 and 0 tensor\n",
        "mask_exiters = tf.equal(exiting_instances, 1) #Mask. Extract only those who exited\n",
        "output = tf.boolean_mask(output, mask_exiters) \n",
        "output"
      ],
      "metadata": {
        "id": "5kt2m4I6le0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorting_tensor = tf.math.multiply(auxiliary_tensor,exiting_instances) #Make zero indexes of Batch that didn't exit\n",
        "mask_non_zeros = tf.not_equal(sorting_tensor, 0) #Take indexes that aren't zero\n",
        "sorting_tensor = tf.boolean_mask(sorting_tensor, mask_non_zeros) #Tensor to sort the output according to instance to which they belonged\n",
        "sorting_tensor"
      ],
      "metadata": {
        "id": "LMDi8G9Vlvvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "auxiliary_tensor = tf.compat.v1.setdiff1d(auxiliary_tensor, sorting_tensor,index_dtype=tf.dtypes.int32)\n",
        "auxiliary_tensor = auxiliary_tensor[0]\n",
        "auxiliary_tensor"
      ],
      "metadata": {
        "id": "_e719d6bl4VV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Appending step. Done after all the exits**"
      ],
      "metadata": {
        "id": "mq__DHP_rqnz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_list.append(output)\n",
        "sorting_list.append(sorting_tensor)"
      ],
      "metadata": {
        "id": "yV5zNyzNl_mD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualizing\n",
        "output_list"
      ],
      "metadata": {
        "id": "PgHgonYFl_9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualizing\n",
        "sorting_list"
      ],
      "metadata": {
        "id": "7XKSCMMbpWqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Done only after all the exits\n",
        "sorting_list = tf.concat(sorting_list, axis=0)\n",
        "sorting_list"
      ],
      "metadata": {
        "id": "l-Kc36D0rbex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorting_idx = tf.argsort(sorting_list)\n",
        "sorting_idx"
      ],
      "metadata": {
        "id": "dITNBqXNpher"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xxx = output_list\n",
        "xxx = tf.concat(xxx, axis=0)\n",
        "xxx\n",
        "#output_list[sorting_idx]"
      ],
      "metadata": {
        "id": "UzrWLSc7rntM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.gather(xxx, sorting_idx, batch_dims = 0)"
      ],
      "metadata": {
        "id": "B0ryZqwRts_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Testing the output stacking for training**"
      ],
      "metadata": {
        "id": "Re-boaJK2yjm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "output = tf.constant([[0.33, 0.33, 0.34],\n",
        "                             [0.9, 0.05, 0.05],\n",
        "                             [0.2, 0.6, 0.2],\n",
        "                              [0.85, 0.05, 0.1]])"
      ],
      "metadata": {
        "id": "996fa-6atauL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "auxiliary_tensor = tf.range(output.shape[0])\n",
        "auxiliary_tensor = tf.reshape(auxiliary_tensor, [-1])\n",
        "auxiliary_tensor"
      ],
      "metadata": {
        "id": "FnUDfrdEta23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_confidence = tf.reduce_max(output, axis = -1) #Highest probs\n",
        "exiting_instances = tf.cast(tf.where(max_confidence < 0.8, x=0, y=1), tf.int32) #Thresholding. 1 and 0 tensor\n",
        "mask_exiters = tf.equal(exiting_instances, 1) #Mask. Extract only those who exited\n",
        "output = tf.boolean_mask(output, mask_exiters) \n",
        "output"
      ],
      "metadata": {
        "id": "8TFaW2BXta-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ccc = tf.stack(output, axis=0)\n",
        "ccc"
      ],
      "metadata": {
        "id": "fQnvl7kH4fpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ttt = tf.stack([ccc,output], axis = 0)\n",
        "ttt"
      ],
      "metadata": {
        "id": "TNUG7szr4l4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ttt[0]"
      ],
      "metadata": {
        "id": "USVfAD4b4u7Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ttt[1]"
      ],
      "metadata": {
        "id": "rpHt_7Rw3CSe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Back to basics..."
      ],
      "metadata": {
        "id": "I_D7-EI_S2Za"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_p = train_dataset_filtered.shuffle(1000).batch(12).map(augment)\n",
        "val_data_p = val_dataset_filtered.batch(12)\n",
        "test_data_p = test_dataset_filtered.batch(12)"
      ],
      "metadata": {
        "id": "uDMNDttQTEKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_conv_block(x, n_filters): #12\n",
        "    # This function applies a simple \"CNN block\" to the input,\n",
        "    # built as Conv2D -> BN -> ReLU -> MaxPool2D.\n",
        "    x = layers.Conv2D(n_filters, 3, padding='same', kernel_regularizer=regularizers.L2(10e-3))(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = tf.nn.relu(x)\n",
        "    return layers.MaxPool2D(2)(x)"
      ],
      "metadata": {
        "id": "gsiaNiHUUcU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classification_layer(x_inp):\n",
        "  x = layers.GlobalAvgPool2D()(x_inp) # Output shape: (None, 32)\n",
        "  x = layers.Dense(100, activation='relu')(x)\n",
        "  x = layers.Dropout(0.3)(x)\n",
        "  x = layers.Dense(10)(x)  \n",
        "  return x"
      ],
      "metadata": {
        "id": "nVsowwI4idJM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def early_exit(x, training = True):\n",
        "  \n",
        "  x = layers.GlobalAvgPool2D()(x) #Possibly have to change names\n",
        "  x = layers.Dense(100, activation='relu')(x)\n",
        "  x = layers.Dropout(0.3)(x)\n",
        "  ee_output = layers.Dense(10, activation='softmax')(x)\n",
        "  return ee_output"
      ],
      "metadata": {
        "id": "zCJ1Vf6GBOSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def early_exit_loss(y_true,y_preds):\n",
        "  scce = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "  loss_ee1 = scce(y_true, y_preds[0])\n",
        "  loss_ee2 = scce(y_true, y_preds[1])\n",
        "  loss_final = scce(y_true, y_preds[2])\n",
        "\n",
        "  return loss_final + loss_ee1*1 + loss_ee2*1"
      ],
      "metadata": {
        "id": "MYK10XrZn0nW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_ee1_res(vect_of_confidences, inp_bef_ee): #Vector of confidences of EE and input of the early exit. Call this only in testing\n",
        "  threshold_ee1 = 0.9 \n",
        "  batch_size = 12\n",
        "  auxiliary_tensor = tf.range(batch_size)\n",
        "  auxiliary_tensor = tf.reshape(auxiliary_tensor, [-1])\n",
        "    \n",
        "  #Take for each image of the Batch the category with highest confidence after the softmax.\n",
        "  max_confidence = tf.reduce_max(vect_of_confidences, axis = -1)\n",
        "\n",
        "  #Thresholding operation. \n",
        "  #New tensor with 0's where confidence is below threshold --> shall be passed to subsequent layers\n",
        "  #1's where confidence is above threshold --> shall NOT be passed to subsequent layers.\n",
        "  exiting_instances = tf.cast(tf.where(max_confidence < threshold_ee1, 0, 1), tf.int32)\n",
        "\n",
        "  #Update output with elements ABOVE threshold that DON'T need to be given to subsequent layers\n",
        "  mask_exiters = tf.equal(exiting_instances, 1) #Mask those elements of batch that took the early exit\n",
        "  output = tf.boolean_mask(vect_of_confidences, mask_exiters) #Take probability vector for the exiter instances\n",
        "\n",
        "  sorting_tensor = tf.math.multiply(auxiliary_tensor,exiting_instances) #Make zero indexes of Batch that didn't exit\n",
        "  mask_non_zeros = tf.not_equal(sorting_tensor, 0) #Take indexes that aren't zero\n",
        "  sorting_tensor = tf.boolean_mask(sorting_tensor, mask_non_zeros) #Tensor to sort the output according to instance to which they belonged\n",
        "\n",
        "  #Update input_non_exiters with elements BELOW threshold that NEED to be given to subsequent layers\n",
        "  mask_non_exiters = tf.equal(exiting_instances, 0)\n",
        "  input_non_exiters = tf.boolean_mask(inp_bef_ee, mask_non_exiters) #多?多?In testing, I update which members of the Batch didn't take EE --> must be passed\n",
        "\n",
        "  #list of number of exiters here\n",
        "\n",
        "  output_list = []\n",
        "  output_list.append(output)\n",
        "\n",
        "  sorting_list = []\n",
        "  sorting_list.append(sorting_tensor)\n",
        "\n",
        "  auxiliary_tensor = tf.compat.v1.setdiff1d(auxiliary_tensor, sorting_tensor,index_dtype=tf.dtypes.int32)\n",
        "  auxiliary_tensor = auxiliary_tensor[0]"
      ],
      "metadata": {
        "id": "VKNnyrE9GDKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model():\n",
        "  # Input part\n",
        "  inp = layers.Input(shape=(500, 375, 3))\n",
        "  # First Convolutional Block     # Output: (None, 500, 375, 24)\n",
        "  x = add_conv_block(inp, 24)     \n",
        "\n",
        "  # Second Convolutional Block    # Output: (None, 250, 187, 48) \n",
        "  x = add_conv_block(x, 48)       \n",
        "\n",
        "  # 1st Early Exit\n",
        "  ee1 = early_exit(x) #These are predictions. Have to save this object in an array\n",
        "  \n",
        "  if training = False: \n",
        "    filter_ee1_res(ee1,x)\n",
        "  # Third Convolutional Block     # Output: (None, 125, 93, 96)\n",
        "  x = add_conv_block(x, 96)\n",
        "\n",
        "  # 2nd Early Exit\n",
        "\n",
        "  # Fourth Convolutional Block    # Output: (None, 62, 46, 192)\n",
        "  x = add_conv_block(x, 192)     \n",
        "\n",
        "  x = classification_layer(x)\n",
        "  return tf.keras.Model(inputs=inp, outputs=x)"
      ],
      "metadata": {
        "id": "q6SoSchZgTxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = build_model()"
      ],
      "metadata": {
        "id": "MTDBoWKrjIBB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net.summary()"
      ],
      "metadata": {
        "id": "WbFfuL8TjKnO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cross_entropy = losses.SparseCategoricalCrossentropy(from_logits=True) #Remove this \"from_logits\" if put the softmax activation in last dense layer\n",
        "accuracy = metrics.SparseCategoricalAccuracy()\n",
        "optimizer = optimizers.Adam()\n",
        "\n",
        "cbs = [\n",
        "    callbacks.TerminateOnNaN(),\n",
        "    callbacks.EarlyStopping(monitor='val_sparse_categorical_accuracy', patience=5, \n",
        "                            restore_best_weights=True, verbose=1),\n",
        "    callbacks.TensorBoard(log_dir='logs', update_freq=50)      \n",
        "]"
      ],
      "metadata": {
        "id": "edffbCKzjL03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net.compile(loss=cross_entropy, optimizer=optimizer, metrics=[accuracy])"
      ],
      "metadata": {
        "id": "bZfSdlx_jVqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net.fit(train_data_p, validation_data=val_data_p, epochs=100, callbacks=cbs)"
      ],
      "metadata": {
        "id": "hGUdElP5jgKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Other possible (failed) approach..."
      ],
      "metadata": {
        "id": "xZHuvOFDjw8M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net = tf.keras.Sequential([\n",
        "    tf.keras.layers.Conv2D(n_filters, 3, padding='same', kernel_regularizer=regularizers.L2(10e-3), input_shape = (500,375,3)),\n",
        "    tf.keras.layers.Dense(50, activation = keras.activations.relu),\n",
        "    tf.keras.layers.Dense(3, activation = keras.activations.softmax)"
      ],
      "metadata": {
        "id": "bTYoicYYfm1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomModel(tf.keras.Model): #New class, inheriting keras.Model.\n",
        "  def __init__(self):\n",
        "    self.net  = [] #Keep track of CB\n",
        "    self.classifiers   = [] #Keep track of EE\n",
        "\n",
        "  def add_conv_struct(self, n_filters, set_shape = False, input_shape=None):\n",
        "    struc_array = []\n",
        "    if set_shape:\n",
        "      struc_array.append(layers.Conv2D(n_filters, 3, padding='same', kernel_regularizer=regularizers.L2(10e-3), input_shape=input_shape))\n",
        "    else:\n",
        "      struc_array.append(layers.Conv2D(n_filters, 3, padding='same', kernel_regularizer=regularizers.L2(10e-3)))\n",
        "    \n",
        "    struc_array.append(layers.BatchNormalization())\n",
        "    struc_array.append(tf.nn.relu())\n",
        "    struc_array.append(layers.MaxPool2D(2))\n",
        "    self.net.append(struc_array)\n",
        "\n",
        "  def classifiers(self):\n",
        "    layers = []\n",
        "    layers.append(layers.GlobalAvgPool2D())\n",
        "    layers.append(layers.Dense(100, activation='relu'))\n",
        "    layers.append(layers.Dropout(0.3))\n",
        "    layers.append(layers.Dense(10, activation='softmax'))\n",
        "    self.classifiers.append(layers)\n",
        "\n",
        "  def build_model(self):\n",
        "  # First Convolutional Block     # Output: (None, 500, 375, 24)\n",
        "  self.add_conv_struct(24, set_shape = True, input_shape = (500, 375, 3))   \n",
        "\n",
        "  # Second Convolutional Block    # Output: (None, 250, 187, 48) \n",
        "  self.add_conv_struct(48)        \n",
        "\n",
        "  # 1st Early Exit\n",
        "  self.classifiers() \n",
        "  \n",
        "  # Third Convolutional Block     # Output: (None, 125, 93, 96)\n",
        "  self.add_conv_struct(96)\n",
        "\n",
        "  # 2nd Early Exit\n",
        "  self.classifiers() \n",
        "\n",
        "  # Fourth Convolutional Block    # Output: (None, 62, 46, 192)\n",
        "  self.add_conv_struct(192)     \n",
        "\n",
        "  # Final Classifier\n",
        "  self.classifiers()\n",
        "\n",
        "  def early_exit_loss(self, y_true,y_preds):\n",
        "    scce = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "    loss = scce(y_true, y_preds)\n",
        "    return loss\n",
        "\n",
        "\n",
        "  def apply(input,layers):\n",
        "    for layer in layers:\n",
        "      input = layer(input)\n",
        "\n",
        "    return input\n",
        "\n",
        "  def fit(self, images, num_epochs):\n",
        "    for epoch in range(num_epochs):\n",
        "      loss = 0\n",
        "      print(f\"Epoch number: \" {epoch})\n",
        "\n",
        "      for batch_idx, (x_batch, y_batch) in enumerate(images):\n",
        "        with tf.GradientTape() as tape:\n",
        "          \n",
        "          output_1 = apply(x_batch, self.net[0]) #First CB\n",
        "          output_2 = apply(output_1,self.net[1]) #Second CB\n",
        "\n",
        "          loss += early_exit_loss(y_batch, apply(output_2,self.classifiers[0])\n",
        "\n",
        "          output_3 = apply(output_2, self.net[2]) #Third CB\n",
        "\n",
        "          loss += early_exit_loss(y_batch, apply(output_3,self.classifiers[1]))\n",
        "\n",
        "          output_4 = apply(output_3, self.net[3]) #Fourth CB\n",
        "\n",
        "          loss += early_exit_loss(y_batch, apply(output_4,self.classifiers[2]))\n",
        "\n",
        "        model = tf.keras.Sequential(self.net[0])\n"
      ],
      "metadata": {
        "id": "H2P-R85xcCYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Original approach - correcting mistakes"
      ],
      "metadata": {
        "id": "fUn5khebrR3I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(training = True):\n",
        "  #*****INPUT******\n",
        "  inp = layers.Input(shape=(500, 375, 3))\n",
        "  #*****FIRST CONVOLUTIONAL BLOCK******\n",
        "\n",
        "  x_cb1_t = add_conv_block(inp, 6)\n",
        "\n",
        "  #*****FIRST EARLY EXIT******\n",
        "  x = layers.GlobalAvgPool2D()(x_cb1_t) #Possibly have to change names\n",
        "  x = layers.Dense(100, activation='relu')(x)\n",
        "  x = layers.Dropout(0.3)(x)\n",
        "  ee1_output = layers.Dense(10, activation='softmax')(x)\n",
        "\n",
        "  piu = tf.stack(ee1_output, axis=0) \n",
        "    \n",
        "  if training == False: #Only during inference\n",
        "\n",
        "    threshold_ee1 = 0.9 \n",
        "    batch_size = 12\n",
        "    auxiliary_tensor = tf.range(batch_size)\n",
        "    auxiliary_tensor = tf.reshape(auxiliary_tensor, [-1])\n",
        "    \n",
        "    #Take for each image of the Batch the category with highest confidence after the softmax.\n",
        "    max_confidence = tf.reduce_max(ee1_output, axis = -1) # TAKING RESULT OF THE EARLY EXIT 1\n",
        "\n",
        "    #Thresholding operation. \n",
        "    #New tensor with 0's where confidence is below threshold --> shall be passed to subsequent layers\n",
        "    #1's where confidence is above threshold --> shall NOT be passed to subsequent layers.\n",
        "    exiting_instances = tf.cast(tf.where(max_confidence < threshold_ee1, 0, 1), tf.int32)\n",
        "\n",
        "    #Update output with elements ABOVE threshold that DON'T need to be given to subsequent layers\n",
        "    mask_exiters = tf.equal(exiting_instances, 1) #Mask those elements of batch that took the early exit\n",
        "    output = tf.boolean_mask(ee1_output, mask_exiters) #Take probability vector for the exiter instances\n",
        "\n",
        "    sorting_tensor = tf.math.multiply(auxiliary_tensor,exiting_instances) #Make zero indexes of Batch that didn't exit\n",
        "    mask_non_zeros = tf.not_equal(sorting_tensor, 0) \n",
        "    sorting_tensor = tf.boolean_mask(sorting_tensor, mask_non_zeros) \n",
        "\n",
        "    #Update input_non_exiters with elements BELOW threshold that NEED to be given to subsequent layers\n",
        "    mask_non_exiters = tf.equal(exiting_instances, 0)\n",
        "    input_non_exiters = tf.boolean_mask(x_cb1_t, mask_non_exiters) \n",
        "\n",
        "    output_list = []\n",
        "    output_list.append(output)\n",
        "\n",
        "    sorting_list = []\n",
        "    sorting_list.append(sorting_tensor)\n",
        "\n",
        "    auxiliary_tensor = tf.compat.v1.setdiff1d(auxiliary_tensor, sorting_tensor,index_dtype=tf.dtypes.int32)\n",
        "    auxiliary_tensor = auxiliary_tensor[0]\n",
        "  #*****FIRST EARLY EXIT******\n",
        "\n",
        "  #*****SECOND CONVOLUTIONAL BLOCK******\n",
        "  if training == True:\n",
        "    input_2cb = x_cb1_t\n",
        "  else: \n",
        "    input_2cb = input_non_exiters\n",
        "\n",
        "  \n",
        "  x_cb2_t = add_conv_block(input_2cb, 6)\n",
        "\n",
        "  #*****SECOND EARLY EXIT******\n",
        "  x = layers.GlobalAvgPool2D()(x_cb2_t) #Possibly have to change names\n",
        "  x = layers.Dense(100, activation='relu')(x)\n",
        "  x = layers.Dropout(0.3)(x)\n",
        "  ee2_output = layers.Dense(10, activation='softmax')(x)\n",
        "\n",
        "  piu = tf.stack([piu,ee2_output], axis=0) \n",
        "\n",
        "  if training == False: #Only during inference\n",
        "\n",
        "    threshold_ee2 = 0.8 #define threshold?\n",
        "    \n",
        "    #Take for each image of the Batch the category with highest confidence after the softmax.\n",
        "    max_confidence = tf.reduce_max(ee2_output, axis = -1)\n",
        "\n",
        "    #Thresholding operation. \n",
        "    #New tensor with 0's where confidence is below threshold --> shall be passed to subsequent layers\n",
        "    #1's where confidence is above threshold --> shall NOT be passed to subsequent layers.\n",
        "    exiting_instances = tf.cast(tf.where(max_confidence < threshold_ee2, x = 0, y = 1), tf.int32)\n",
        "\n",
        "    #Update output with elements ABOVE threshold that DON'T need to be given to subsequent layers\n",
        "    mask_exiters = tf.equal(exiting_instances, 1) #Mask those elements of batch that took the early exit\n",
        "    output = tf.boolean_mask(ee2_output, mask_exiters) #Take probability vector for the exiter instances\n",
        "\n",
        "    sorting_tensor = tf.math.multiply(auxiliary_tensor,exiting_instances) #Make zero indexes of Batch that didn't exit\n",
        "    mask_non_zeros = tf.not_equal(sorting_tensor, 0) #Take indexes that aren't zero\n",
        "    sorting_tensor = tf.boolean_mask(sorting_tensor, mask_non_zeros) #Tensor to sort the output according to instance to which they belonged\n",
        "\n",
        "    #Update input_non_exiters with elements BELOW threshold that NEED to be given to subsequent layers\n",
        "    mask_non_exiters = tf.equal(exiting_instances, 0)\n",
        "    input_non_exiters = tf.boolean_mask(x_cb2_t, mask_non_exiters) #多?多?In testing, I update which members of the Batch didn't take EE --> must be passed\n",
        "\n",
        "    #Appending operation\n",
        "    output_list.append(output)\n",
        "    sorting_list.append(sorting_tensor)\n",
        "\n",
        "    #Auxiliary tensor update --> possibly dispensable\n",
        "    auxiliary_tensor = tf.compat.v1.setdiff1d(auxiliary_tensor, sorting_tensor,index_dtype=tf.dtypes.int32)\n",
        "    auxiliary_tensor = auxiliary_tensor[0]\n",
        "                                    \n",
        "\n",
        "    #*****FINAL EXIT******\n",
        "  if training == True:\n",
        "    input_final_exit = x_cb2_t\n",
        "  else: \n",
        "    input_final_exit = input_non_exiters\n",
        "\n",
        "  x = layers.GlobalAvgPool2D()(input_final_exit) \n",
        "  x = layers.Dense(100, activation='relu')(x)\n",
        "  x = layers.Dropout(0.3)(x)\n",
        "  final_output = layers.Dense(10, activation='softmax')(x)\n",
        "  \n",
        "  final_out = tf.expand_dims(final_output, axis=0)\n",
        "  piu = tf.concat([piu, final_out], axis=0)                       \n",
        "  assert piu.shape == (3, None, 10)\n",
        "  x = piu\n",
        "  '''\n",
        "  if training == False:\n",
        "\n",
        "    #Appending Operation\n",
        "    output_list.append(final_output)\n",
        "    sorting_list.append(sorting_tensor)\n",
        "    \n",
        "    #Sorting operation\n",
        "    sorting_list = tf.concat(sorting_list, axis=0)\n",
        "    sorting_idx = tf.argsort(sorting_list)\n",
        "    output_list = tf.concat(output_list, axis=0)\n",
        "\n",
        "    x = tf.gather(output_list, sorting_idx, batch_dims = 0)\n",
        "  '''\n",
        "\n",
        "  return tf.keras.Model(inputs=inp, outputs=x)"
      ],
      "metadata": {
        "id": "KTPKXoMvYT4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model()"
      ],
      "metadata": {
        "id": "s3i-j6FEoskr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "WEnfhXf2pIEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def early_exit_loss(y_true,y_preds):\n",
        "  scce = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "  loss_ee1 = scce(y_true, y_preds[0])\n",
        "  print(y_true.shape)\n",
        "  print(y_preds.shape)\n",
        "  loss_ee2 = scce(y_true, y_preds[1])\n",
        "  loss_final = scce(y_true, y_preds[2])\n",
        "\n",
        "  return loss_final + loss_ee1*1 + loss_ee2*1"
      ],
      "metadata": {
        "id": "-XHX-h--pHfA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#cross_entropy = losses.SparseCategoricalCrossentropy(from_logits=True) #Remove this \"from_logits\" if put the softmax activation in last dense layer\n",
        "accuracy = metrics.SparseCategoricalAccuracy()\n",
        "optimizer = optimizers.Adam()\n",
        "\n",
        "# Callbacks are objects that provide additional functionalities during training,\n",
        "# allowing to plug-in things at will (in this case, we add a callback to immediately\n",
        "# terminate when a NaN value is encountered, a callback to perform early stopping,\n",
        "# and a callback to log the results for TensorBoard visualization).\n",
        "cbs = [\n",
        "    callbacks.TerminateOnNaN(),\n",
        "    callbacks.EarlyStopping(monitor='val_sparse_categorical_accuracy', patience=5, \n",
        "                            restore_best_weights=True, verbose=1),\n",
        "    callbacks.TensorBoard(log_dir='logs', update_freq=50)      \n",
        "]"
      ],
      "metadata": {
        "id": "tKw_ifNRo6Df"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=early_exit_loss, optimizer=optimizer, metrics=[accuracy])"
      ],
      "metadata": {
        "id": "VI4pEXLFqJOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_data_p, validation_data=val_data_p, epochs=1, callbacks=cbs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T2Kk0LuxpvGg",
        "outputId": "0025c6b7-0a4c-4f0b-d5db-2eb4a7074f7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(None,)\n",
            "(3, None, 10)\n",
            "(None,)\n",
            "(3, None, 10)\n",
            "     67/Unknown - 31s 78ms/step - loss: 7.0423 - sparse_categorical_accuracy: 0.1053(None,)\n",
            "(3, None, 10)\n",
            "67/67 [==============================] - 37s 163ms/step - loss: 7.0423 - sparse_categorical_accuracy: 0.1053 - val_loss: 6.9951 - val_sparse_categorical_accuracy: 0.1100\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f48e908e970>"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.predict(test_data_p)"
      ],
      "metadata": {
        "id": "1psUgbwLxG4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(test_data_p)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T2I6AfQnqfz8",
        "outputId": "c8f26092-81c5-4357-cf03-7a5f592e3ec1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9/9 [==============================] - 3s 357ms/step - loss: 6.9846 - sparse_categorical_accuracy: 0.0774\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[6.98460578918457, 0.07744107395410538]"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "iGdZCK_NuH70",
        "fUMasSh8_5Ht",
        "DvPbILO5BjKx",
        "_1tYo_L2_Nyk",
        "psmbn-VWi9wI",
        "y6Y6fdtyW2vp",
        "Re-boaJK2yjm"
      ]
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}